{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9617fa52-bb21-430b-9940-2e615e08cdcc",
   "metadata": {},
   "source": [
    "### Module 1: Infrastructure & Core Ingestion\n",
    "#### 1.1 Libraries & Dependencies\n",
    "This section initializes the geospatial environment. We isolate all library imports in a single atomic cell to ensure that dependencies are loaded once at the beginning of the notebook. This separates the environment setup from the data ingestion logic and prevents redundant executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97170236-bc73-4bca-9528-7d873998b8c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Geospatial Operations & Visualization\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgeopandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgpd\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "# Data Manipulation & Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Geospatial Operations & Visualization\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Spatial Logic\n",
    "from shapely.geometry import Point, LineString\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# System & Path Operations\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39cd37c-152c-4e54-ae87-ac7b5bc31d92",
   "metadata": {},
   "source": [
    "#### 1.2 Global Configurations & CRS Selection\n",
    "This cell centralizes our file paths, conversion factors, and geospatial constants to ensure consistency across the notebook. \n",
    "\n",
    "**Trial & Error Note:** Raw geographic data is almost always provided in `EPSG:4326` (WGS 84, using latitude and longitude degrees). We cannot calculate accurate physical distances (e.g., \"miles to the nearest train station\") using degrees because the physical distance of a degree changes depending on latitude. To solve this, we define our target Coordinate Reference System (CRS) as `EPSG:3435` (NAD83 / Illinois East). This local state plane projection measures distance in US Survey Feet, providing the high-precision planar geometry required for urban distance calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fccfdc-0c5d-4037-b5f3-84d1d85748fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory Paths\n",
    "DATA_RAW_DIR = \"../../data_raw\"\n",
    "DATA_PROCESSED_DIR = \"../../data_processed\"\n",
    "\n",
    "# Geospatial Constants\n",
    "# EPSG:3435 is the specific projection for Chicago/Illinois East (unit: feet)\n",
    "TARGET_CRS = \"EPSG:3435\"\n",
    "# EPSG:4326 is the standard GPS coordinate system (unit: degrees)\n",
    "RAW_CRS = \"EPSG:4326\"\n",
    "\n",
    "# Mathematical Constants\n",
    "FEET_TO_MILES = 1 / 5280.0\n",
    "\n",
    "print(f\"Configurations loaded. Raw data directory: {DATA_RAW_DIR}\")\n",
    "print(f\"Target CRS strictly set to: {TARGET_CRS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d04e86-6385-4cc7-a6b5-1e23b857334f",
   "metadata": {},
   "source": [
    "#### 1.3 Ingest Airbnb Listings Data\n",
    "Here we load the raw Airbnb listings from the CSV file and immediately convert them into a spatial `GeoDataFrame`.\n",
    "\n",
    "**Trial & Error Note:** When creating the `GeoDataFrame`, we must first explicitly define the initial coordinate reference system as `RAW_CRS` (`EPSG:4326`) because the CSV provides `latitude` and `longitude` in degrees. If we skip this step, GeoPandas won't know how to interpret the raw numbers. Once defined, we chain the `.to_crs()` method to immediately project the data into our `TARGET_CRS` (`EPSG:3435`) for accurate distance calculations in subsequent modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e89400-0d59-4d16-b8a3-3188e79f2081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define specific file path for listings\n",
    "LISTINGS_PATH = os.path.join(DATA_RAW_DIR, \"listings.csv\")\n",
    "\n",
    "print(\"--- Loading Airbnb Listings ---\")\n",
    "\n",
    "# Load raw CSV\n",
    "df_raw = pd.read_csv(LISTINGS_PATH)\n",
    "\n",
    "# Convert to GeoDataFrame and project to Target CRS\n",
    "listings_gdf = gpd.GeoDataFrame(\n",
    "    df_raw, \n",
    "    geometry=gpd.points_from_xy(df_raw.longitude, df_raw.latitude),\n",
    "    crs=RAW_CRS\n",
    ").to_crs(TARGET_CRS)\n",
    "\n",
    "print(f\"âœ… Success: Loaded {len(listings_gdf)} listings and projected to {TARGET_CRS}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c56a9f-8c83-403a-b4fd-681749f33b1b",
   "metadata": {},
   "source": [
    "#### 1.4 Spatial Join & Boundary Validation\n",
    "To ensure our dataset strictly contains properties within Chicago city limits, we ingest the official neighborhood boundaries and perform a spatial join (`sjoin`). \n",
    "\n",
    "**Trial & Error Note:** Raw Airbnb datasets frequently include listings from surrounding suburbs (like Evanston or Oak Park) that are improperly tagged as \"Chicago\". By executing a Point-in-Polygon spatial join against the official city shapefile, we effectively create a spatial filter. Listings that fail to match a neighborhood are identified as out-of-bounds and dropped. The final visualization confirms that all remaining properties fall perfectly within the city's jurisdiction.\n",
    "[Image of GIS spatial join point in polygon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c085e24-d4c7-4546-a75a-1e59c4de6eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Ingest Neighborhood Boundaries\n",
    "# Load and immediately project to TARGET_CRS (EPSG:3435) to match listings\n",
    "NEIGHBORHOODS_PATH = os.path.join(DATA_RAW_DIR, \"neighbourhoods.geojson\")\n",
    "neighborhoods_gdf = gpd.read_file(NEIGHBORHOODS_PATH).to_crs(TARGET_CRS)\n",
    "\n",
    "# 1. Standardize Neighborhood GeoDataFrame\n",
    "# Keep only essential columns and rename for clarity in the final dataset\n",
    "neighborhoods_standardized = neighborhoods_gdf[['neighbourhood', 'geometry']].rename(\n",
    "    columns={'neighbourhood': 'official_neighbourhood'}\n",
    ")\n",
    "\n",
    "# 2. Spatial Join: Listings to Neighborhoods\n",
    "# Use 'left' join to identify listings that don't fall within any official boundary\n",
    "listings_cleaned = gpd.sjoin(\n",
    "    listings_gdf, \n",
    "    neighborhoods_standardized, \n",
    "    how='left', \n",
    "    predicate='within'\n",
    ")\n",
    "\n",
    "# 3. Data Cleaning: Remove out-of-bounds listings\n",
    "initial_count = len(listings_cleaned)\n",
    "listings_cleaned = listings_cleaned.dropna(subset=['official_neighbourhood'])\n",
    "\n",
    "print(\"--- Spatial Join & Cleaning ---\")\n",
    "print(f\"âœ… Success: Spatial join complete using 'official_neighbourhood'.\")\n",
    "print(f\"ðŸ§¹ Removed {initial_count - len(listings_cleaned)} listings found outside city boundaries.\")\n",
    "\n",
    "# 4. Visualization: Neighborhood Distribution\n",
    "# Plot the official boundaries as the base layer, with verified listings on top\n",
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "\n",
    "# Base layer: Neighborhood polygons\n",
    "neighborhoods_standardized.plot(\n",
    "    ax=ax, \n",
    "    color='#f2f2f2', \n",
    "    edgecolor='#999999', \n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "# Top layer: Cleaned Airbnb listings\n",
    "listings_cleaned.plot(\n",
    "    ax=ax, \n",
    "    markersize=2, \n",
    "    color='#3498db', \n",
    "    alpha=0.6, \n",
    "    label='Verified Listings'\n",
    ")\n",
    "\n",
    "ax.set_title(\"Map 1: Chicago Airbnb Listings & Neighborhood Boundaries\", fontsize=14)\n",
    "ax.set_axis_off()\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d50027-d519-486a-8143-173556e0242b",
   "metadata": {},
   "source": [
    "### Module 2: Spatial Reference Points & Distance Metrics\n",
    "#### 2.1 Define The Loop Center & Calculate Distance\n",
    "The Chicago Loop is the central business district and a primary anchor for real estate and short-term rental pricing. We calculate the linear distance from each listing to the absolute center of the Chicago grid (State St & Madison St).\n",
    "\n",
    "**Trial & Error Note:** A common mistake in spatial analysis is calculating the distance between a raw coordinate (in degrees) and a projected GeoDataFrame (in feet). This will yield nonsensical results. We must first define the Loop center as a Shapely `Point` in `EPSG:4326`, project it into our `TARGET_CRS` (`EPSG:3435`), and *then* calculate the distance. Finally, we convert the resulting measurement from US Survey Feet to miles for better interpretability in our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fd465e-942e-4e20-9219-8d72129d82ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Chicago Loop Center (State St & Madison St)\n",
    "# Coordinates in standard GPS format (Longitude, Latitude)\n",
    "loop_center_raw = Point(-87.6278, 41.8820)\n",
    "\n",
    "# 2. Project the reference point to TARGET_CRS\n",
    "# We wrap it in a GeoSeries to utilize GeoPandas' built-in projection method\n",
    "loop_center_proj = gpd.GeoSeries([loop_center_raw], crs=RAW_CRS).to_crs(TARGET_CRS).iloc[0]\n",
    "\n",
    "# 3. Calculate Distance\n",
    "# Calculate distance in feet, then apply our global conversion factor to miles\n",
    "listings_cleaned['dist_loop_center_miles'] = listings_cleaned.geometry.distance(loop_center_proj) * FEET_TO_MILES\n",
    "\n",
    "print(\"--- Distance to Loop Calculation ---\")\n",
    "print(f\"âœ… Success: 'dist_loop_center_miles' calculated.\")\n",
    "\n",
    "# 4. Quick validation of the new feature\n",
    "# Ensure 'id' exists as a unique identifier, and check the first 5 distance results\n",
    "if 'id' in listings_cleaned.columns:\n",
    "    print(listings_cleaned[['id', 'dist_loop_center_miles']].head())\n",
    "else:\n",
    "    # Fallback if 'id' is named differently (e.g., 'listing_id')\n",
    "    print(listings_cleaned[['dist_loop_center_miles']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14107459-350e-4e6d-a562-542f149f5f5f",
   "metadata": {},
   "source": [
    "### Module 2: Spatial Reference Points & Distance Metrics\n",
    "#### 2.1 Exploring Distances to Major City Landmarks\n",
    "Initially, we hypothesize that proximity to several major commercial and tourist hubsâ€”not just the Loopâ€”might dictate Airbnb pricing. To test this, we calculate the linear distance from each listing to a curated set of key Chicago landmarks.\n",
    "\n",
    "**Trial & Error Note:** Why calculate all these distances if we might not use them all in the final model? In urban spatial analysis, landmarks clustered near the downtown area (like Navy Pier, Museum Campus, and the Loop) often exhibit severe multicollinearity. By calculating exact distances to all of them now, we can later evaluate their correlation and spatial overlap. This exploration allows us to empirically decide which continuous distances to keep (e.g., the Loop) and which might be better simplified into a binary proximity flag (e.g., \"is within walking distance to *any* landmark\") to reduce model noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775bde0f-67e8-4d27-bb86-496836465123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define key landmarks: (Latitude, Longitude)\n",
    "landmarks_raw = {\n",
    "    'loop_center': (41.8781, -87.6298),\n",
    "    'navy_pier': (41.8917, -87.6047),\n",
    "    'ohare_airport': (41.9742, -87.9073),\n",
    "    'fulton_market': (41.8868, -87.6483),\n",
    "    'museum_campus': (41.8641, -87.6140),\n",
    "    'mccormick_place': (41.8512, -87.6171)\n",
    "}\n",
    "\n",
    "print(\"--- Calculating Distances to Major Landmarks ---\")\n",
    "\n",
    "# 2. Iterate through landmarks, project, and calculate distance\n",
    "for name, (lat, lon) in landmarks_raw.items():\n",
    "    # Create Shapely Point (Note: Shapely requires x=longitude, y=latitude)\n",
    "    pt_raw = Point(lon, lat)\n",
    "    \n",
    "    # Project the point from RAW_CRS (EPSG:4326) to TARGET_CRS (EPSG:3435)\n",
    "    pt_proj = gpd.GeoSeries([pt_raw], crs=RAW_CRS).to_crs(TARGET_CRS).iloc[0]\n",
    "    \n",
    "    # Calculate distance in feet, then convert to miles\n",
    "    col_name = f'dist_{name}_miles'\n",
    "    listings_cleaned[col_name] = listings_cleaned.geometry.distance(pt_proj) * FEET_TO_MILES\n",
    "    \n",
    "    print(f\"âœ… Calculated feature: {col_name}\")\n",
    "\n",
    "# 3. Validation: Check the first 5 rows of the newly created distance columns\n",
    "dist_cols = [f'dist_{name}_miles' for name in landmarks_raw.keys()]\n",
    "print(\"\\nSample of calculated distances (in miles):\")\n",
    "display(listings_cleaned[dist_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084fa1ba-73a3-48d5-aa30-f45f640832ce",
   "metadata": {},
   "source": [
    "#### 2.2 Exploratory Correlation: Distances vs. Price by Room Type\n",
    "Before blindly incorporating all calculated landmark distances into our final model, we must evaluate their actual predictive power regarding our target variable, `price`. We segment this analysis by `room_type` because the geographical preferences of tourists booking entire homes often differ starkly from individuals renting private rooms.\n",
    "\n",
    "**Trial & Error Note:** This step reveals two critical insights. \n",
    "First, we will likely observe that distances to downtown landmarks (Loop, Navy Pier, Museum Campus) exhibit strong negative correlations with price for 'Entire home/apt', meaning prices drop as you move further away. However, 'Private room' prices tend to be inherently capped and less geographically elastic, showing weaker correlations. \n",
    "Second, because these downtown landmarks are clustered together, their correlations are almost identical. Keeping all of them would introduce severe multicollinearity. This justifies our ultimate strategy: keeping only `dist_loop_center_miles` as the continuous baseline proxy for downtown proximity, and later converting the rest into a boolean interaction feature (`is_landmark_proximate`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c394da-752c-4053-9fd2-f87d8c5a77f0",
   "metadata": {},
   "source": [
    "#### 2.2 Exploratory Correlation: Distances vs. Price (Heatmap)\n",
    "Before finalizing which landmark distances to incorporate into our model, we must evaluate their actual predictive power regarding our target variable, `price`. We segment this analysis by `room_type` using a correlation heatmap, as the geographical sensitivities of tourists booking entire homes often differ starkly from individuals renting private rooms.\n",
    "\n",
    "**Trial & Error Note:** The heatmap format provides a dense, immediate comparison of correlation intensities. We can clearly observe that distances to downtown landmarks exhibit much stronger negative correlations (darker blue/red depending on the colormap) for 'Entire home/apt' compared to 'Private room'. Crucially, `dist_navy_pier_miles` emerges as the strongest single continuous predictor among the group, justifying our decision to pivot away from the Loop as our primary distance anchor. Furthermore, the nearly identical correlation scores across the downtown landmarks highlight severe multicollinearity, prompting us to keep only one continuous metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3989d0bf-3169-4da5-b292-aba3387e15ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Clean 'price' column (Airbnb raw prices often contain '$' and ',')\n",
    "if listings_cleaned['price'].dtype == 'O':\n",
    "    listings_cleaned['price_num'] = pd.to_numeric(\n",
    "        listings_cleaned['price'].astype(str).str.replace(r'[\\$,]', '', regex=True), \n",
    "        errors='coerce'\n",
    "    )\n",
    "else:\n",
    "    listings_cleaned['price_num'] = listings_cleaned['price']\n",
    "\n",
    "# 2. Define the columns we want to analyze\n",
    "dist_cols = [f'dist_{name}_miles' for name in landmarks_raw.keys()]\n",
    "room_types = ['Entire home/apt', 'Private room']\n",
    "\n",
    "print(\"--- Correlation Analysis: Distance vs. Price ---\")\n",
    "\n",
    "# 3. Build a correlation matrix specifically for the heatmap\n",
    "corr_data = {}\n",
    "for r_type in room_types:\n",
    "    # Filter dataset by room type\n",
    "    subset = listings_cleaned[listings_cleaned['room_type'] == r_type]\n",
    "    \n",
    "    # Calculate Pearson correlation with numerical price\n",
    "    corrs = subset[dist_cols + ['price_num']].corr()['price_num'].drop('price_num')\n",
    "    corr_data[r_type] = corrs\n",
    "\n",
    "# Convert the dictionary into a DataFrame for Seaborn\n",
    "corr_df = pd.DataFrame(corr_data)\n",
    "\n",
    "# 4. Visualization: Heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Use 'coolwarm' cmap: Red for positive corr, Blue for negative corr\n",
    "sns.heatmap(\n",
    "    corr_df, \n",
    "    annot=True,          # Show exact correlation values\n",
    "    cmap='coolwarm', \n",
    "    fmt=\".3f\",           # 3 decimal places\n",
    "    linewidths=0.5, \n",
    "    cbar_kws={'label': 'Pearson Correlation'}\n",
    ")\n",
    "\n",
    "plt.title(\"Correlation: Landmark Distances vs. Price by Room Type\", fontsize=14, pad=15)\n",
    "plt.ylabel(\"Landmark Distance Features\")\n",
    "plt.xlabel(\"Room Type\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Cleanup acknowledgement\n",
    "if 'price_num' in listings_cleaned.columns and listings_cleaned['price'].dtype == 'O':\n",
    "    print(\"âœ… Price numeric conversion successful and correlations plotted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795bcb77-3b94-44da-ab27-da74c5fdfb06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Clean 'price' column (Airbnb raw prices often contain '$' and ',')\n",
    "if listings_cleaned['price'].dtype == 'O':\n",
    "    listings_cleaned['price_num'] = pd.to_numeric(\n",
    "        listings_cleaned['price'].astype(str).str.replace(r'[\\$,]', '', regex=True), \n",
    "        errors='coerce'\n",
    "    )\n",
    "else:\n",
    "    listings_cleaned['price_num'] = listings_cleaned['price']\n",
    "\n",
    "# 2. Define the columns we want to analyze\n",
    "dist_cols = [f'dist_{name}_miles' for name in landmarks_raw.keys()]\n",
    "room_types = ['Entire home/apt', 'Private room']\n",
    "\n",
    "print(\"--- Correlation Analysis: Distance vs. Price ---\")\n",
    "\n",
    "# 3. Create a side-by-side visualization for the two room types\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "\n",
    "for i, r_type in enumerate(room_types):\n",
    "    # Filter dataset by room type\n",
    "    subset = listings_cleaned[listings_cleaned['room_type'] == r_type]\n",
    "    \n",
    "    # Calculate Pearson correlation with numerical price\n",
    "    corrs = subset[dist_cols + ['price_num']].corr()['price_num'].drop('price_num')\n",
    "    \n",
    "    # Sort for better visualization readability\n",
    "    corrs_sorted = corrs.sort_values()\n",
    "    \n",
    "    # Plot horizontal bar chart (Red for negative corr, Green for positive)\n",
    "    colors = ['#e74c3c' if val < 0 else '#2ecc71' for val in corrs_sorted]\n",
    "    corrs_sorted.plot(kind='barh', ax=axes[i], color=colors, edgecolor='black', alpha=0.8)\n",
    "    \n",
    "    axes[i].set_title(f\"Correlation with Price: {r_type}\", fontsize=12)\n",
    "    axes[i].set_xlabel(\"Pearson Correlation Coefficient\")\n",
    "    axes[i].axvline(0, color='black', linewidth=1)\n",
    "    axes[i].grid(axis='x', linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.suptitle(\"Impact of Landmark Proximity on Price by Room Type\", fontsize=15, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Cleanup: Drop the temporary numeric price if it wasn't the original\n",
    "if 'price_num' in listings_cleaned.columns and listings_cleaned['price'].dtype == 'O':\n",
    "    # Keep it for future steps if needed, but usually good to acknowledge\n",
    "    print(\"âœ… Price numeric conversion successful and correlations plotted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac572e3c-8988-40f4-9a82-f98cb36b57da",
   "metadata": {},
   "source": [
    "#### 2.2 Exploratory Correlation & Multicollinearity Check\n",
    "Before finalizing which landmark distances to incorporate into our model, we must evaluate two things: their predictive power regarding our target variable (`price`), and their correlation with each other (multicollinearity). We segment this analysis by `room_type` using side-by-side full correlation heatmaps.\n",
    "\n",
    "**Trial & Error Note:** By examining the full correlation matrices, two critical insights emerge. \n",
    "First, we observe severe multicollinearity among the downtown landmarks. The distances to the Loop, Navy Pier, Museum Campus, and Fulton Market are highly correlated with one another (often > 0.8). Including all of them would severely confound a regression model. \n",
    "Second, looking at the `price_num` row/column, `dist_navy_pier_miles` consistently shows the strongest negative correlation with price, particularly for 'Entire home/apt'. This data-driven evidence perfectly justifies our strategy: we will keep Navy Pier as our sole continuous downtown proximity anchor and avoid the multicollinearity trap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9146c257-2132-4de6-a16b-7fed768282fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Clean 'price' column (Airbnb raw prices often contain '$' and ',')\n",
    "if listings_cleaned['price'].dtype == 'O':\n",
    "    listings_cleaned['price_num'] = pd.to_numeric(\n",
    "        listings_cleaned['price'].astype(str).str.replace(r'[\\$,]', '', regex=True), \n",
    "        errors='coerce'\n",
    "    )\n",
    "else:\n",
    "    listings_cleaned['price_num'] = listings_cleaned['price']\n",
    "\n",
    "# 2. Define the columns for the correlation matrix\n",
    "dist_cols = [f'dist_{name}_miles' for name in landmarks_raw.keys()]\n",
    "analysis_cols = dist_cols + ['price_num']\n",
    "room_types = ['Entire home/apt', 'Private room']\n",
    "\n",
    "print(\"--- Multicollinearity & Price Correlation Analysis ---\")\n",
    "\n",
    "# 3. Visualization: Side-by-side full correlation heatmaps\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "for i, r_type in enumerate(room_types):\n",
    "    # Filter dataset by room type\n",
    "    subset = listings_cleaned[listings_cleaned['room_type'] == r_type]\n",
    "    \n",
    "    # Calculate the full Pearson correlation matrix\n",
    "    corr_matrix = subset[analysis_cols].corr()\n",
    "    \n",
    "    # Create a mask to hide the upper triangle for better readability (optional but professional)\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(\n",
    "        corr_matrix, \n",
    "        mask=mask,\n",
    "        annot=True,          # Show exact correlation values\n",
    "        cmap='coolwarm',     # Red for positive, Blue for negative\n",
    "        fmt=\".2f\",           # 2 decimal places\n",
    "        vmin=-1, vmax=1,     # Fix scale from -1 to 1\n",
    "        linewidths=0.5, \n",
    "        ax=axes[i],\n",
    "        cbar_kws={\"shrink\": 0.8}\n",
    "    )\n",
    "    \n",
    "    axes[i].set_title(f\"Correlation Matrix: {r_type}\", fontsize=14, pad=15)\n",
    "    \n",
    "    # Rotate x-axis labels to prevent overlap\n",
    "    axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Cleanup acknowledgement\n",
    "if 'price_num' in listings_cleaned.columns and listings_cleaned['price'].dtype == 'O':\n",
    "    print(\"âœ… Price numeric conversion successful. Full correlation matrices plotted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f39e466-c8f9-4a1b-999c-48ef3ad5cd9f",
   "metadata": {},
   "source": [
    "#### 2.3 Feature Selection Decision: Heatmap Insights & Multicollinearity\n",
    "Based on the full correlation heatmaps above, we observe severe multicollinearity among Chicago's downtown landmarks (The Loop, Navy Pier, Museum Campus, Fulton Market, and McCormick Place). Their inter-variable correlation coefficients are extremely high. Retaining all of them as continuous distance metrics would introduce significant instability and noise into any future pricing model. \n",
    "\n",
    "To resolve this, we must select a single spatial anchor for downtown proximity. The heatmap clearly reveals that `dist_navy_pier_miles` exhibits the strongest negative correlation with listing prices, particularly in the 'Entire home/apt' segment. Thus, we pivot from our initial baseline (`dist_loop_center_miles`) and retain Navy Pier. \n",
    "\n",
    "Furthermore, we must address O'Hare Airport (`dist_ohare_airport_miles`). Although it is geographically isolated and does *not* suffer from multicollinearity with the downtown cluster, the heatmap demonstrates that its direct explanatory power regarding price is exceptionally weak. Consequently, we also discard it as a continuous predictor.\n",
    "\n",
    "**Trial & Error Note:** Rather than entirely losing the spatial signals of these discarded landmarks, we will repurpose their underlying data in the next step. By consolidating them into a simplified boolean interaction feature (`is_landmark_proximate`), we can still capture the general \"Nearest Attraction\" premium for the model without falling into the multicollinearity trap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140a8df7-776b-4171-ab0e-8e6d8565ee1c",
   "metadata": {},
   "source": [
    "#### 2.4 Feature Engineering: Landmark Proximity & Gravity\n",
    "While `dist_navy_pier_miles` captures the primary downtown premium, we still want to acknowledge listings that might be far from Navy Pier but are immediately adjacent to other hubs (like O'Hare Airport or Fulton Market). To achieve this without introducing multicollinearity, we consolidate the remaining landmark distances into a \"Nearest Attraction\" effect.\n",
    "\n",
    "**Trial & Error Note:** Passing six highly correlated distance features to a model often dilutes their individual importance. Instead, we extract the minimum distance to *any* landmark. From this, we engineer two new features: a binary proximity flag (`is_landmark_proximate`, thresholded at 1.5 miles to represent a reasonable walking/short transit radius) and a non-linear `landmark_gravity_score`. The gravity score (`1 / (distance + 1)`) linearizes the \"exponential\" value of prime locations, testing the hypothesis that the price premium spikes dramatically only when you are extremely close to an attraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc277aa7-d45b-4a24-a2b2-7ccd4b7402e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Define the list of landmark distance columns created in Module 2.1\n",
    "landmark_dist_cols = [\n",
    "    'dist_loop_center_miles', 'dist_navy_pier_miles', 'dist_ohare_airport_miles',\n",
    "    'dist_fulton_market_miles', 'dist_museum_campus_miles', 'dist_mccormick_place_miles'\n",
    "]\n",
    "\n",
    "# 2. Minimum Distance to ANY Landmark\n",
    "# Represents the listing's strongest geographic selling point\n",
    "listings_cleaned['min_dist_to_any_landmark_miles'] = listings_cleaned[landmark_dist_cols].min(axis=1)\n",
    "\n",
    "# 3. Binary Proximity Flag (Threshold: 1.5 miles)\n",
    "# Identifies \"Landmark-proximate\" listings regardless of which specific hub they are near\n",
    "listings_cleaned['is_landmark_proximate'] = (listings_cleaned['min_dist_to_any_landmark_miles'] <= 1.5).astype(int)\n",
    "\n",
    "# 4. Gravity Score Transformation\n",
    "# Values near 1 mean \"extremely close\", near 0 mean \"isolated\"\n",
    "listings_cleaned['landmark_gravity_score'] = 1 / (listings_cleaned['min_dist_to_any_landmark_miles'] + 1)\n",
    "\n",
    "print(\"--- Updated Landmark Feature Correlation ---\")\n",
    "\n",
    "# 5. Validation: Correlation Check using the numeric price calculated in step 2.2\n",
    "new_features = ['price_num', 'min_dist_to_any_landmark_miles', 'is_landmark_proximate', 'landmark_gravity_score']\n",
    "new_corr = listings_cleaned[new_features].corr()['price_num'].sort_values(ascending=False)\n",
    "print(new_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d922677d-8037-4eff-8962-51c435869387",
   "metadata": {},
   "source": [
    "#### 2.5 Validation: Engineered Landmark Features Heatmap\n",
    "To confirm that our new consolidated features effectively capture the spatial premium, we evaluate their correlation with price and with one another, split by room type. Using a full correlation heatmap ensures methodological consistency with our earlier baseline analysis.\n",
    "\n",
    "**Trial & Error Note:** As expected, `landmark_gravity_score` (which non-linearly rewards extreme proximity) shows a strong positive correlation with price, while `min_dist_to_any_landmark_miles` shows a negative correlation. Furthermore, the heatmap prominently displays high structural collinearity among these three engineered features themselves (e.g., proximity flags and gravity scores are mathematically derived from the same base distance). This visual confirmation reminds us that while these features elegantly capture the \"Nearest Attraction\" effect, we must not use all three simultaneously in a linear regression model to avoid redundant variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f39421-4c30-48a7-bc09-cd13615876e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Define the newly engineered features alongside our target variable\n",
    "engineered_features = [\n",
    "    'min_dist_to_any_landmark_miles', \n",
    "    'is_landmark_proximate', \n",
    "    'landmark_gravity_score',\n",
    "    'price_num' # Included to observe impact on price\n",
    "]\n",
    "room_types = ['Entire home/apt', 'Private room']\n",
    "\n",
    "print(\"--- Heatmap: Engineered Features vs. Price ---\")\n",
    "\n",
    "# 2. Create side-by-side full correlation heatmaps\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for i, r_type in enumerate(room_types):\n",
    "    # Filter dataset by room type\n",
    "    subset = listings_cleaned[listings_cleaned['room_type'] == r_type]\n",
    "    \n",
    "    # Calculate the full Pearson correlation matrix\n",
    "    corr_matrix = subset[engineered_features].corr()\n",
    "    \n",
    "    # Create a mask to hide the upper triangle for visual clarity\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(\n",
    "        corr_matrix, \n",
    "        mask=mask,\n",
    "        annot=True,          # Show exact correlation values\n",
    "        cmap='coolwarm',     # Red for positive, Blue for negative\n",
    "        fmt=\".2f\",           # 2 decimal places\n",
    "        vmin=-1, vmax=1,     # Fix scale from -1 to 1\n",
    "        linewidths=0.5, \n",
    "        ax=axes[i],\n",
    "        cbar_kws={\"shrink\": 0.8}\n",
    "    )\n",
    "    \n",
    "    axes[i].set_title(f\"Engineered Features: {r_type}\", fontsize=13, pad=15)\n",
    "    \n",
    "    # Rotate x-axis labels to prevent overlap\n",
    "    axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=30, ha='right')\n",
    "\n",
    "plt.suptitle(\"Impact of Engineered Landmark Features on Price by Room Type\", fontsize=15, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406178fe-5a21-470c-aef8-fd01cdb3af1f",
   "metadata": {},
   "source": [
    "#### 2.6 Final Feature Selection Decision: Resolving Multicollinearity\n",
    "Based on the comprehensive heatmap analysis, we observe severe multicollinearity not only among the individual downtown landmarks (e.g., Navy Pier, The Loop) but also between our engineered features (like the `landmark_gravity_score`). Including all these overlapping spatial signals would severely destabilize any regression model.\n",
    "\n",
    "**Trial & Error Note:** To definitively resolve the multicollinearity trap while preserving the critical \"Nearest Attraction\" premium, we must drastically prune our features. We have decided to drop all individual landmark distances (including our initial baseline `dist_loop_center_miles` and the highly correlated `dist_navy_pier_miles`), as well as the `landmark_gravity_score`. Moving forward, we will exclusively retain **`min_dist_to_any_landmark_miles`** (as a generalized continuous proxy for downtown proximity) and **`is_landmark_proximate`** (as a sharp, non-linear binary flag for prime locations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b785342f-d49e-4ce1-9cb9-0b55a8ad558f",
   "metadata": {},
   "source": [
    "#### 2.6 Final Collinearity Check: Navy Pier vs. Engineered Features\n",
    "Before fully discarding `dist_navy_pier_miles`â€”which earlier proved to be our strongest individual predictorâ€”we must empirically verify that our new engineered features effectively capture its spatial variance without introducing redundancy. We achieve this by plotting a focused correlation heatmap comparing the legacy feature alongside our new metrics.\n",
    "\n",
    "**Trial & Error Note:** The resulting heatmap provides the definitive evidence for our feature pruning strategy. We observe a remarkably high positive correlation between `dist_navy_pier_miles` and `min_dist_to_any_landmark_miles`. This indicates that our consolidated \"minimum distance\" feature successfully absorbs the geographic signal of Navy Pier. Retaining both in a linear model would be redundant. Therefore, we confidently drop `dist_navy_pier_miles` and proceed strictly with `min_dist_to_any_landmark_miles` and the non-linear `is_landmark_proximate` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33d22b8-e419-454b-b80c-48f420ac8a3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Define the specific features for our final collinearity showdown\n",
    "final_check_features = [\n",
    "    'dist_navy_pier_miles',             # The best original landmark\n",
    "    'min_dist_to_any_landmark_miles',   # Our new continuous proxy\n",
    "    'is_landmark_proximate',            # Our new boolean flag\n",
    "    'price_num'                         # Target variable\n",
    "]\n",
    "\n",
    "room_types = ['Entire home/apt', 'Private room']\n",
    "\n",
    "print(\"--- Final Collinearity Check: Legacy vs. Engineered Features ---\")\n",
    "\n",
    "# 2. Create side-by-side correlation heatmaps\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for i, r_type in enumerate(room_types):\n",
    "    # Filter dataset by room type\n",
    "    subset = listings_cleaned[listings_cleaned['room_type'] == r_type]\n",
    "    \n",
    "    # Calculate the Pearson correlation matrix\n",
    "    corr_matrix = subset[final_check_features].corr()\n",
    "    \n",
    "    # Create a mask to hide the upper triangle\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(\n",
    "        corr_matrix, \n",
    "        mask=mask,\n",
    "        annot=True,          \n",
    "        cmap='coolwarm',     \n",
    "        fmt=\".2f\",           \n",
    "        vmin=-1, vmax=1,     \n",
    "        linewidths=0.5, \n",
    "        ax=axes[i],\n",
    "        cbar_kws={\"shrink\": 0.8}\n",
    "    )\n",
    "    \n",
    "    axes[i].set_title(f\"Collinearity Check: {r_type}\", fontsize=13, pad=15)\n",
    "    \n",
    "    # Rotate x-axis labels to prevent overlap\n",
    "    axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=25, ha='right')\n",
    "\n",
    "plt.suptitle(\"Collinearity Showdown: Navy Pier vs. Engineered Features\", fontsize=15, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fa8789-1278-45be-b522-e31847fb034f",
   "metadata": {},
   "source": [
    "#### 2.7 Feature Pruning: Executing the Final Selection\n",
    "Based on the collinearity showdown in the previous heatmap, we observe extreme multicollinearity between our engineered `min_dist_to_any_landmark_miles` and our strongest legacy feature, `dist_navy_pier_miles`. Retaining both is redundant. \n",
    "\n",
    "**Trial & Error Note:** In applied data science, interpretability is just as important as mathematical elegance. Explaining a price premium based on \"distance to Navy Pier\" is highly intuitive for stakeholders, whereas \"minimum distance to an arbitrary set of landmarks\" can be abstract. Therefore, we finalize our decision: we will retain `dist_navy_pier_miles` as our continuous downtown anchor and `is_landmark_proximate` as our boolean premium flag. We will now formally drop all other calculated landmark distances and intermediate engineered features to clean our dataset and prevent data leakage or multicollinearity in future modeling phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b686d6b0-2f8f-4d66-904c-6c09876c543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Executing Feature Pruning ---\")\n",
    "\n",
    "# 1. Define the list of features to drop based on our multicollinearity analysis\n",
    "cols_to_drop = [\n",
    "    'dist_loop_center_miles',\n",
    "    'dist_ohare_airport_miles',\n",
    "    'dist_fulton_market_miles',\n",
    "    'dist_museum_campus_miles',\n",
    "    'dist_mccormick_place_miles',\n",
    "    'min_dist_to_any_landmark_miles',\n",
    "    'landmark_gravity_score'\n",
    "]\n",
    "\n",
    "# 2. Safely drop columns if they exist in the dataframe\n",
    "existing_cols_to_drop = [col for col in cols_to_drop if col in listings_cleaned.columns]\n",
    "listings_cleaned = listings_cleaned.drop(columns=existing_cols_to_drop)\n",
    "\n",
    "# 3. Validation\n",
    "print(f\"ðŸ§¹ Successfully dropped {len(existing_cols_to_drop)} redundant features to resolve multicollinearity.\")\n",
    "print(\"âœ… Retained critical spatial features: 'dist_navy_pier_miles' and 'is_landmark_proximate'.\")\n",
    "\n",
    "# Display a subset of the current columns to confirm\n",
    "retained_check = [col for col in ['dist_navy_pier_miles', 'is_landmark_proximate'] if col in listings_cleaned.columns]\n",
    "print(\"\\nRetained Landmark Features Head:\")\n",
    "display(listings_cleaned[retained_check].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a14d58-c624-410e-acc5-fa114a81e45d",
   "metadata": {},
   "source": [
    "#### 2.8 Ingest Rail Transit Data (API & Local)\n",
    "To analyze transit accessibility, we aggregate data from Chicago's two rail systems. Since CTA data is managed by the city, we fetch the live **CTA 'L' Station** data directly from the **Chicago Data Portal API**. For **Metra**, we use the local shapefile provided in the project directory.\n",
    "\n",
    "**Trial & Error Note:** Fetching data via API ensures we are working with the most recent station metadata. However, the raw API data is in `EPSG:4326` (lat/lon). We must immediately project it to `EPSG:3435` (US Survey Feet) so that it aligns with our listings and allows for accurate distance calculations in the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e1b0f4-d6d8-4d3c-a7b0-d38085c53ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Resource Paths\n",
    "# CTA L-Stations from Chicago Data Portal (GeoJSON API)\n",
    "CTA_API_URL = \"https://data.cityofchicago.org/resource/8pix-ypme.geojson\"\n",
    "METRA_SHP_PATH = os.path.join(DATA_RAW_DIR, \"Metra_Stations/MetraStations.shp\")\n",
    "\n",
    "print(\"--- Loading Transit Datasets ---\")\n",
    "\n",
    "# 2. Fetch CTA Stations from API\n",
    "try:\n",
    "    # Directly reading from URL into GeoDataFrame\n",
    "    cta_stations_gdf = gpd.read_file(CTA_API_URL).to_crs(TARGET_CRS)\n",
    "    print(f\"âœ… Success: Fetched {len(cta_stations_gdf)} CTA stations via API.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error fetching CTA data: {e}\")\n",
    "\n",
    "# 3. Ingest local Metra Stations\n",
    "try:\n",
    "    metra_gdf = gpd.read_file(METRA_SHP_PATH).to_crs(TARGET_CRS)\n",
    "    print(f\"âœ… Success: Loaded {len(metra_gdf)} Metra stations locally.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading Metra data: {e}\")\n",
    "\n",
    "# 4. Visualization: Integrated Rail Map\n",
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "\n",
    "# Layer 1: Neighborhoods\n",
    "neighborhoods_gdf.plot(ax=ax, color='#f2f2f2', edgecolor='#cccccc', alpha=0.5)\n",
    "\n",
    "# Layer 2: CTA (API Data)\n",
    "if 'cta_stations_gdf' in locals():\n",
    "    cta_stations_gdf.plot(ax=ax, markersize=12, color='#1e90ff', label='CTA (API)', alpha=0.7)\n",
    "\n",
    "# Layer 3: Metra (Local Data)\n",
    "if 'metra_gdf' in locals():\n",
    "    metra_gdf.plot(ax=ax, markersize=12, color='#ff4757', marker='s', label='Metra (Local)', alpha=0.7)\n",
    "\n",
    "ax.set_title(\"Map 2: Chicago Rail Infrastructure (API + Local)\", fontsize=14)\n",
    "ax.set_axis_off()\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79595010-bf40-443a-b200-c980ac87dc91",
   "metadata": {},
   "source": [
    "#### 2.9 High-Performance Transit Engineering & Interaction Terms\n",
    "In this section, we move beyond simple \"closest station\" metrics to capture the true complexity of Chicago's transit landscape. We hypothesize that a listing's value is driven by two advanced spatial factors:\n",
    "\n",
    "1. **Transit Redundancy ($k=3$):** A property near multiple stations is more resilient and valuable than one near a single isolated stop. We use a `cKDTree` to efficiently query the three nearest stations and calculate their average distance.\n",
    "2. **The \"North Side\" Premium:** Chicagoâ€™s market exhibits a strong socio-economic gradient. By normalizing latitude (`lat_norm`) and multiplying it by our transit and landmark proximity scores, we create **Interaction Terms**. \n",
    "\n",
    "**Trial & Error Note:** Why `k=3`? In urban centers, a single station closure is mitigated if other lines are nearby. This \"redundancy\" is a key premium for business travelers and tourists. Furthermore, we implement `transit_north_premium` to test if the \"Interaction\" between being in the North Side and being near a landmark creates a price effect greater than the sum of its parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ade164-aa2e-486b-9436-e39430d6aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Unify CTA and Metra Rail Data\n",
    "# We use .centroid to ensure we are calculating point-to-point distances \n",
    "print(\"--- Unifying Rail Infrastructure for Spatial Indexing ---\")\n",
    "\n",
    "# Combine CTA and Metra into a single high-performance reference set\n",
    "rail_coords_list = []\n",
    "if 'cta_stations_gdf' in locals():\n",
    "    rail_coords_list.append(cta_stations_gdf.geometry.centroid)\n",
    "if 'metra_gdf' in locals():\n",
    "    rail_coords_list.append(metra_gdf.geometry.centroid)\n",
    "\n",
    "# Create a unified numpy array of station coordinates (X, Y) in EPSG:3435\n",
    "rail_points = pd.concat(rail_coords_list)\n",
    "rail_coords_array = np.array(list(rail_points.apply(lambda p: (p.x, p.y))))\n",
    "\n",
    "# 2. Build cKDTree and Query Top 3 Neighbors\n",
    "# We query k=3 to capture the transit 'density' of the area\n",
    "tree = cKDTree(rail_coords_array)\n",
    "listing_coords = np.array(list(listings_cleaned.geometry.apply(lambda p: (p.x, p.y))))\n",
    "rail_dists, _ = tree.query(listing_coords, k=3)\n",
    "\n",
    "# 3. Apply Distance Logic (Converting Feet to Miles)\n",
    "# rail_dists[:, 0] is the nearest station; np.mean(rail_dists, axis=1) is the top 3 average\n",
    "listings_cleaned['min_dist_rail_miles'] = rail_dists[:, 0] * FEET_TO_MILES\n",
    "listings_cleaned['avg_dist_top3_rail_miles'] = np.mean(rail_dists, axis=1) * FEET_TO_MILES\n",
    "listings_cleaned['is_rail_accessible'] = (listings_cleaned['min_dist_rail_miles'] <= 0.5).astype(int)\n",
    "\n",
    "# 4. Latitude Normalization & Interaction Terms\n",
    "# Normalize latitude: 0 = South-most, 1 = North-most\n",
    "min_lat = listings_cleaned['latitude'].min()\n",
    "max_lat = listings_cleaned['latitude'].max()\n",
    "listings_cleaned['lat_norm'] = (listings_cleaned['latitude'] - min_lat) / (max_lat - min_lat)\n",
    "\n",
    "# Re-calculate a temporary Gravity Score for the interaction (using Navy Pier as the anchor)\n",
    "# This captures the 'synergy' between being near a landmark and being in the North\n",
    "# Formula: 1 / (d + 1)\n",
    "temp_gravity = 1 / (listings_cleaned['dist_navy_pier_miles'] + 1)\n",
    "listings_cleaned['transit_north_premium'] = temp_gravity * listings_cleaned['lat_norm']\n",
    "\n",
    "# Premium Transit Hub: Listings that are BOTH accessible (<=0.5mi) AND in the North\n",
    "listings_cleaned['is_premium_transit_hub'] = (listings_cleaned['is_rail_accessible'] * listings_cleaned['lat_norm'])\n",
    "\n",
    "print(f\"âœ… Success: Engineered 5 new transit & interaction features.\")\n",
    "print(f\"Sample - Transit North Premium: {listings_cleaned['transit_north_premium'].mean():.4f}\")\n",
    "\n",
    "# 5. Quick Correlation check with Price\n",
    "transit_cols = [\n",
    "    'min_dist_rail_miles', 'avg_dist_top3_rail_miles', 'is_rail_accessible', \n",
    "    'transit_north_premium', 'is_premium_transit_hub', 'price_num'\n",
    "]\n",
    "display(listings_cleaned[transit_cols].corr()['price_num'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d6624a-9150-4f5c-b022-57d86c1e6aea",
   "metadata": {},
   "source": [
    "#### 2.10 Validating Transit Features: Segmented Heatmap Analysis\n",
    "To evaluate the effectiveness of our engineered transit features, we analyze their correlation with `price_num` across different room types. This ensures our features capture market-specific nuancesâ€”specifically the hypothesis that premium transit access is a high-value driver for entire apartment rentals but potentially less critical for budget-conscious private room seekers.\n",
    "\n",
    "**Trial & Error Note:** By including the interaction terms (`transit_north_premium`, `is_premium_transit_hub`) in this full correlation matrix, we can also monitor for internal multicollinearity. We expect to see that while `min_dist_rail_miles` and `avg_dist_top3_rail_miles` are naturally correlated, our interaction terms provide a distinct \"geographic-weighted\" signal that may offer superior predictive power without simply duplicating raw distance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0809605-4b80-4617-b6ee-3c66d5caf0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. Define the transit-specific features for analysis\n",
    "transit_analysis_features = [\n",
    "    'min_dist_rail_miles', \n",
    "    'avg_dist_top3_rail_miles', \n",
    "    'is_rail_accessible',\n",
    "    'transit_north_premium', \n",
    "    'is_premium_transit_hub',\n",
    "    'price_num'\n",
    "]\n",
    "\n",
    "room_types = ['Entire home/apt', 'Private room']\n",
    "\n",
    "print(\"--- Heatmap: Transit & Interaction Features vs. Price ---\")\n",
    "\n",
    "# 2. Create side-by-side correlation heatmaps\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "for i, r_type in enumerate(room_types):\n",
    "    # Filter dataset by room type\n",
    "    subset = listings_cleaned[listings_cleaned['room_type'] == r_type]\n",
    "    \n",
    "    # Calculate the full Pearson correlation matrix\n",
    "    corr_matrix = subset[transit_analysis_features].corr()\n",
    "    \n",
    "    # Create a mask to hide the upper triangle\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(\n",
    "        corr_matrix, \n",
    "        mask=mask,\n",
    "        annot=True,          # Show exact correlation values\n",
    "        cmap='coolwarm',     # Red for positive, Blue for negative\n",
    "        fmt=\".2f\",           # 2 decimal places\n",
    "        vmin=-1, vmax=1,     # Fix scale from -1 to 1\n",
    "        linewidths=0.5, \n",
    "        ax=axes[i],\n",
    "        cbar_kws={\"shrink\": 0.8}\n",
    "    )\n",
    "    \n",
    "    axes[i].set_title(f\"Transit Impact: {r_type}\", fontsize=14, pad=15)\n",
    "    \n",
    "    # Rotate x-axis labels to prevent overlap\n",
    "    axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=30, ha='right')\n",
    "\n",
    "plt.suptitle(\"Multicollinearity & Price Signal: Integrated Transit Features\", fontsize=16, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d36243-81f2-45f5-be3a-67e5e4b8e144",
   "metadata": {},
   "source": [
    "#### 2.11 Feature Convergence: Prioritizing Geographic-Transit Synergy\n",
    "Following our multicollinearity audit, we have reached a definitive conclusion regarding our transit features. The interaction termsâ€”`transit_north_premium` and `is_premium_transit_hub`â€”effectively capture not just the proximity to rail, but the high-value \"synergy\" between transit access and premium North Side locations. \n",
    "\n",
    "**Trial & Error Note:** Why drop the raw distance metrics? \n",
    "1. **Multicollinearity:** `min_dist_rail_miles` and `avg_dist_top3_rail_miles` are highly collinear with our interaction terms. \n",
    "2. **Explanatory Power:** The heatmaps confirm that the latitude-weighted features offer a much sharper signal for price prediction. \n",
    "By pruning the redundant raw metrics and the intermediate `lat_norm`, we ensure our model remains parsimonious (simple yet powerful) and avoids the mathematical instability caused by overlapping spatial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d27127-b3fe-4e5b-9e3e-d8d556093613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the transit features and intermediate variables to drop\n",
    "print(\"--- Finalizing Transit Feature Selection ---\")\n",
    "\n",
    "transit_cols_to_drop = [\n",
    "    'min_dist_rail_miles', \n",
    "    'avg_dist_top3_rail_miles', \n",
    "    'is_rail_accessible', \n",
    "    'lat_norm'\n",
    "]\n",
    "\n",
    "# 2. Execute dropping\n",
    "existing_transit_drops = [c for c in transit_cols_to_drop if c in listings_cleaned.columns]\n",
    "listings_cleaned = listings_cleaned.drop(columns=existing_transit_drops)\n",
    "\n",
    "# 3. Validation\n",
    "print(f\"ðŸ§¹ Successfully pruned {len(existing_transit_drops)} redundant transit features.\")\n",
    "print(\"âœ… Retained high-signal features: 'transit_north_premium' and 'is_premium_transit_hub'.\")\n",
    "\n",
    "# Quick look at the remaining geographic columns\n",
    "remaining_geo_cols = [\n",
    "    'dist_navy_pier_miles', \n",
    "    'is_landmark_proximate', \n",
    "    'transit_north_premium', \n",
    "    'is_premium_transit_hub'\n",
    "]\n",
    "display(listings_cleaned[remaining_geo_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d733bb68-838a-4c24-b352-eedc0a987480",
   "metadata": {},
   "source": [
    "#### 2.12 The Lakefront Effect: Shoreline & Beach Premium\n",
    "The Michigan lakefront is Chicago's ultimate real estate anchor. In this section, we differentiate between the general **Shoreline** (the potential for a lake view) and specific **Beaches** (recreational utility). \n",
    "\n",
    "**Trial & Error Note:** 1. **Shoreline Gravity:** Why use $1 / (d + 1)$? Linear distance doesn't capture the \"cliff-edge\" effect of lakefront pricing. The premium for being 0.1 miles from the lake is exponentially higher than being 1.1 miles away. The gravity score linearizes this relationship for our model.\n",
    "2. **The Golden Quadrant:** By multiplying `shoreline_gravity` with our `lat_norm`, we create the **`shoreline_north_premium`**. This feature specifically highlights listings in the \"Golden Quadrant\"â€”the North Side lakefrontâ€”which historically commands the highest ADR (Average Daily Rate) in the Chicago Airbnb market.\n",
    "\n",
    "[Image of Chicago lakefront skyline and beaches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185fba3e-be5f-48b0-9840-607236f239ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Shoreline Geometry (LineString approximation for Chicago Coast)\n",
    "# These coordinates represent the general bend of the Lake Michigan shoreline\n",
    "print(\"--- Engineering Lakefront & Beach Features ---\")\n",
    "\n",
    "shoreline_pts = [\n",
    "    (-87.6625, 42.0190), (-87.6350, 41.9400), (-87.6200, 41.8910), \n",
    "    (-87.6060, 41.8500), (-87.5750, 41.7900), (-87.5300, 41.7100)\n",
    "]\n",
    "shoreline_line = LineString([(p[0], p[1]) for p in shoreline_pts])\n",
    "\n",
    "# Project Shoreline to TARGET_CRS\n",
    "shoreline_gs = gpd.GeoSeries([shoreline_line], crs=\"EPSG:4326\").to_crs(TARGET_CRS).iloc[0]\n",
    "\n",
    "# 2. Define Specific High-Value Beach Points\n",
    "beaches = {\n",
    "    'north_ave_beach': (41.9148, -87.6244),\n",
    "    'oak_street_beach': (41.9038, -87.6225),\n",
    "    '57th_st_beach': (41.7915, -87.5788) \n",
    "}\n",
    "\n",
    "# Convert beach points to projected geometries\n",
    "beach_geoms = [\n",
    "    gpd.GeoSeries([Point(c[1], c[0])], crs=\"EPSG:4326\").to_crs(TARGET_CRS).iloc[0] \n",
    "    for c in beaches.values()\n",
    "]\n",
    "\n",
    "# 3. Calculate Distances & Gravity\n",
    "# Recalculate lat_norm (since we pruned it in 2.11) to use as a weighting factor\n",
    "min_lat = listings_cleaned['latitude'].min()\n",
    "max_lat = listings_cleaned['latitude'].max()\n",
    "lat_norm = (listings_cleaned['latitude'] - min_lat) / (max_lat - min_lat)\n",
    "\n",
    "# Distance to general shoreline (miles)\n",
    "listings_cleaned['dist_to_shoreline_miles'] = listings_cleaned.geometry.distance(shoreline_gs) * FEET_TO_MILES\n",
    "\n",
    "# Distance to nearest specific beach (miles)\n",
    "listings_cleaned['dist_to_nearest_beach_miles'] = listings_cleaned.geometry.apply(\n",
    "    lambda x: min([x.distance(bg) for bg in beach_geoms])\n",
    ") * FEET_TO_MILES\n",
    "\n",
    "# Shoreline Gravity Score: 1 / (dist + 1)\n",
    "# Using a 1-mile decay constant for intuitive scaling\n",
    "listings_cleaned['shoreline_gravity'] = 1 / (listings_cleaned['dist_to_shoreline_miles'] + 1)\n",
    "\n",
    "# 4. Interaction Term: Shoreline North Premium\n",
    "# This captures the compounding value of being near the lake AND in the North Side\n",
    "listings_cleaned['shoreline_north_premium'] = listings_cleaned['shoreline_gravity'] * lat_norm\n",
    "\n",
    "print(f\"âœ… Success: Engineered 4 lakefront premium features.\")\n",
    "print(f\"Max Shoreline North Premium: {listings_cleaned['shoreline_north_premium'].max():.4f}\")\n",
    "\n",
    "# 5. Quick Check: Head of new features\n",
    "display(listings_cleaned[['dist_to_shoreline_miles', 'dist_to_nearest_beach_miles', \n",
    "                          'shoreline_gravity', 'shoreline_north_premium']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d2e76a-c83f-4777-a912-ea27a1590fc3",
   "metadata": {},
   "source": [
    "#### 2.13 Validating Lakefront Premiums: Correlation & Multi-collinearity\n",
    "With our shoreline and beach features engineered, we must evaluate their individual and collective impact on price. We are particularly interested in seeing if the non-linear `shoreline_gravity` and the interaction term `shoreline_north_premium` outperform the raw linear distances (`dist_to_shoreline_miles`). \n",
    "\n",
    "**Trial & Error Note:** In Chicago, lakefront proximity and latitude are the two most powerful spatial price drivers. However, they often move together (multi-collinearity). By plotting these heatmaps, we can see if `shoreline_north_premium` captures enough unique variance to justify dropping the redundant raw distance metrics. We expect to see a much stronger positive correlation with price for 'Entire home/apt' listings compared to 'Private rooms', as premium views and beach access are luxury-tier drivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc3c09c-9113-4cfe-ad90-f6aeff373be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. Define the shoreline-related features for analysis\n",
    "shoreline_features = [\n",
    "    'dist_to_shoreline_miles', \n",
    "    'dist_to_nearest_beach_miles', \n",
    "    'shoreline_gravity', \n",
    "    'shoreline_north_premium',\n",
    "    'price_num'\n",
    "]\n",
    "\n",
    "room_types = ['Entire home/apt', 'Private room']\n",
    "\n",
    "print(\"--- Heatmap: Lakefront Features vs. Price ---\")\n",
    "\n",
    "# 2. Create side-by-side correlation heatmaps\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "for i, r_type in enumerate(room_types):\n",
    "    # Filter dataset by room type\n",
    "    subset = listings_cleaned[listings_cleaned['room_type'] == r_type]\n",
    "    \n",
    "    # Calculate the Pearson correlation matrix\n",
    "    corr_matrix = subset[shoreline_features].corr()\n",
    "    \n",
    "    # Create a mask to hide the upper triangle\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(\n",
    "        corr_matrix, \n",
    "        mask=mask,\n",
    "        annot=True,          # Show exact correlation values\n",
    "        cmap='coolwarm',     # Red for positive, Blue for negative\n",
    "        fmt=\".2f\",           # 2 decimal places\n",
    "        vmin=-1, vmax=1,     # Fix scale from -1 to 1\n",
    "        linewidths=0.5, \n",
    "        ax=axes[i],\n",
    "        cbar_kws={\"shrink\": 0.8}\n",
    "    )\n",
    "    \n",
    "    axes[i].set_title(f\"Lakefront Impact: {r_type}\", fontsize=14, pad=15)\n",
    "    \n",
    "    # Rotate x-axis labels to prevent overlap\n",
    "    axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=30, ha='right')\n",
    "\n",
    "plt.suptitle(\"Price Signal & Collinearity: Lakefront Premium Features\", fontsize=16, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dc9f5c-5d24-4ba4-9f95-9dfc69dfce0c",
   "metadata": {},
   "source": [
    "#### 2.14 Final Geospatial Pruning: Optimizing Lakefront Features\n",
    "After reviewing the collinearity patterns, we have finalized our lakefront feature set. We observe that while `shoreline_north_premium` captures a powerful regional signal, it is highly collinear with our primary `shoreline_gravity` score. To maintain a robust and interpretable model, we will prioritize the non-linear gravity metric over the interaction term.\n",
    "\n",
    "**Trial & Error Note:** Why these two? \n",
    "1. **`shoreline_gravity`**: Captures the \"view and prestige\" premium which decays rapidly as one moves inland. \n",
    "2. **`dist_to_nearest_beach_miles`**: Captures a distinct \"recreational utility\" signal that differs from mere shoreline proximity. \n",
    "By dropping the raw `dist_to_shoreline_miles` (redundant with gravity) and the interaction `shoreline_north_premium`, we achieve a high-signal, low-noise representation of Chicago's most valuable geographic asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dbfd9c-a733-4b13-8aff-891ebd196b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the lakefront features to drop to resolve multicollinearity\n",
    "print(\"--- Finalizing Lakefront Feature Selection ---\")\n",
    "\n",
    "lakefront_cols_to_drop = [\n",
    "    'dist_to_shoreline_miles', \n",
    "    'shoreline_north_premium'\n",
    "]\n",
    "\n",
    "# 2. Execute dropping\n",
    "existing_lake_drops = [c for c in lakefront_cols_to_drop if c in listings_cleaned.columns]\n",
    "listings_cleaned = listings_cleaned.drop(columns=existing_lake_drops)\n",
    "\n",
    "# 3. Validation and Final Dataset Audit\n",
    "print(f\"ðŸ§¹ Successfully pruned {len(existing_lake_drops)} redundant lakefront features.\")\n",
    "print(\"âœ… Retained high-signal features: 'shoreline_gravity' and 'dist_to_nearest_beach_miles'.\")\n",
    "\n",
    "# Final Audit of all engineered geospatial features currently in the dataset\n",
    "final_geo_summary = [\n",
    "    'dist_navy_pier_miles', \n",
    "    'is_landmark_proximate', \n",
    "    'transit_north_premium', \n",
    "    'is_premium_transit_hub',\n",
    "    'shoreline_gravity',\n",
    "    'dist_to_nearest_beach_miles'\n",
    "]\n",
    "\n",
    "print(\"\\n--- Current Geospatial Feature Set ---\")\n",
    "display(listings_cleaned[final_geo_summary].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e8bbee-b41b-4a68-ad56-5264a6cd346c",
   "metadata": {},
   "source": [
    "#### 3.1 Multi-Scale Safety Matrix: Comprehensive Density and Risk\n",
    "In this final iteration of safety engineering, we expand our metrics to include **Total Crime** alongside specialized categories. This dual-scale approach allows us to distinguish between the general \"urban intensity\" of a neighborhood and specific \"safety threats.\" \n",
    "\n",
    "**Trial & Error Note:** 1. **Total vs. Categorical:** While `nb_crimes_half_mile_total` captures the overall activity level of an area (which sometimes correlates with high-traffic, high-price urban centers), the `weighted_safety_risk_score` isolates the true psychological deterrents.\n",
    "2. **The 50:10:1 Logic:** We maintain the aggressive weighting for homicides (50x) and violent crimes (10x). By subtracting homicides from the violent count, we ensure that the most catastrophic incidents are weighted heavily without being counted twice in the final score.\n",
    "3. **Density Normalization:** Every neighborhood-level metric is normalized by `area_sq_miles`, ensuring that our \"reputation\" signals are based on concentration rather than just the size of the administrative boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c592fc5c-9f7c-430d-9061-04c64250262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Neighborhood Data Preparation: Calculate Area\n",
    "print(\"--- Calculating Neighborhood Areas ---\")\n",
    "# EPSG:3435 (feet) -> sq feet to sq miles\n",
    "neighborhoods_gdf['area_sq_miles'] = neighborhoods_gdf.geometry.area / (5280**2)\n",
    "\n",
    "# 2. Neighborhood-Level Aggregation (Densities)\n",
    "print(\"--- Aggregating Crime Densities to Neighborhoods ---\")\n",
    "\n",
    "# Define Categories\n",
    "violent_types = ['HOMICIDE', 'ROBBERY', 'BATTERY', 'CRIM SEXUAL ASSAULT', 'ASSAULT']\n",
    "property_types = ['THEFT', 'BURGLARY', 'MOTOR VEHICLE THEFT', 'DECEPTIVE PRACTICE']\n",
    "\n",
    "# Subset crimes for focused processing\n",
    "crimes_homicide = crimes_gdf[crimes_gdf['primary_type'] == 'HOMICIDE']\n",
    "crimes_violent = crimes_gdf[crimes_gdf['primary_type'].isin(violent_types)]\n",
    "crimes_property = crimes_gdf[crimes_gdf['primary_type'].isin(property_types)]\n",
    "\n",
    "# Spatial Join: Assign crimes to neighborhoods\n",
    "crimes_with_nbh = gpd.sjoin(crimes_gdf, neighborhoods_gdf[['neighbourhood', 'geometry', 'area_sq_miles']], how='inner', predicate='within')\n",
    "\n",
    "# Create the summary table\n",
    "nbh_final_stats = crimes_with_nbh.groupby('neighbourhood').agg(\n",
    "    total_crime=('primary_type', 'count'),\n",
    "    violent=('primary_type', lambda x: x.isin(violent_types).sum()),\n",
    "    property=('primary_type', lambda x: x.isin(property_types).sum())\n",
    ")\n",
    "\n",
    "# Join area back and calculate all 3 density metrics\n",
    "nbh_final_stats = nbh_final_stats.join(neighborhoods_gdf.set_index('neighbourhood')['area_sq_miles'])\n",
    "nbh_final_stats['crime_density_total'] = nbh_final_stats['total_crime'] / nbh_final_stats['area_sq_miles']\n",
    "nbh_final_stats['crime_density_violent'] = nbh_final_stats['violent'] / nbh_final_stats['area_sq_miles']\n",
    "nbh_final_stats['crime_density_property'] = nbh_final_stats['property'] / nbh_final_stats['area_sq_miles']\n",
    "\n",
    "# Map densities to listings\n",
    "listings_cleaned['crime_density_total'] = listings_cleaned['neighbourhood'].map(nbh_final_stats['crime_density_total'])\n",
    "listings_cleaned['crime_density_violent'] = listings_cleaned['neighbourhood'].map(nbh_final_stats['crime_density_violent'])\n",
    "listings_cleaned['crime_density_property'] = listings_cleaned['neighbourhood'].map(nbh_final_stats['crime_density_property'])\n",
    "\n",
    "# 3. Listing-Level Aggregation (0.5-Mile Buffer Counts)\n",
    "print(\"--- Calculating 0.5-Mile Buffer Metrics ---\")\n",
    "listings_buffer = gpd.GeoDataFrame(geometry=listings_cleaned.geometry.buffer(2640), crs=TARGET_CRS)\n",
    "\n",
    "# Utility to count points in buffer\n",
    "def count_in_buffer(buffer_df, points_gdf):\n",
    "    join = gpd.sjoin(buffer_df, points_gdf[['geometry']], how='left', predicate='intersects')\n",
    "    return join.groupby(join.index)['index_right'].count()\n",
    "\n",
    "listings_cleaned['nb_homicides_half_mile'] = count_in_buffer(listings_buffer, crimes_homicide)\n",
    "listings_cleaned['nb_crimes_half_mile_violent'] = count_in_buffer(listings_buffer, crimes_violent)\n",
    "listings_cleaned['nb_crimes_half_mile_property'] = count_in_buffer(listings_buffer, crimes_property)\n",
    "listings_cleaned['nb_crimes_half_mile_total'] = count_in_buffer(listings_buffer, crimes_gdf)\n",
    "\n",
    "# 4. Final Weighted Safety Risk Score\n",
    "print(\"--- Finalizing Weighted Safety Risk Score ---\")\n",
    "# Formula: (Homicide * 50) + ((Violent - Homicide) * 10) + (Property * 1)\n",
    "listings_cleaned['weighted_safety_risk_score'] = (\n",
    "    (listings_cleaned['nb_homicides_half_mile'] * 50) + \n",
    "    ((listings_cleaned['nb_crimes_half_mile_violent'] - listings_cleaned['nb_homicides_half_mile']) * 10) + \n",
    "    (listings_cleaned['nb_crimes_half_mile_property'] * 1)\n",
    ")\n",
    "\n",
    "print(\"âœ… Success: Full safety matrix generated.\")\n",
    "safety_final_cols = [\n",
    "    'crime_density_total', 'crime_density_violent', 'crime_density_property',\n",
    "    'nb_crimes_half_mile_total', 'nb_crimes_half_mile_violent', 'nb_crimes_half_mile_property',\n",
    "    'nb_homicides_half_mile', 'weighted_safety_risk_score'\n",
    "]\n",
    "display(listings_cleaned[safety_final_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011f8202-301f-4477-8b13-9ac17741b8f5",
   "metadata": {},
   "source": [
    "#### 3.2 Visualizing the \"Safety Discount\": Crime Correlations\n",
    "In this section, we analyze how our newly engineered safety metrics correlate with listing prices. We segment this analysis by room type to test if \"Entire Home\" guests (who may be families or luxury travelers) are more sensitive to safety signals than \"Private Room\" guests (who are often more budget-conscious).\n",
    "\n",
    "**Trial & Error Note:** 1. **Signal Strength:** Look closely at `weighted_safety_risk_score`. Because it amplifies homicides and violent crimes, it should show a stronger negative correlation with price than raw `total_crime` or `property_crime` counts.\n",
    "2. **Atmosphere vs. Proximity:** We compare `crime_density_violent` (neighborhood reputation) with `nb_crimes_half_mile_violent` (immediate danger). If the neighborhood density has a stronger correlation, it suggests that the \"area's reputation\" drives price more than the actual incidents happening at the doorstep.\n",
    "3. **Multicollinearity Check:** Many of these features will be highly correlated (e.g., where there is violent crime, there is often property crime). This heatmap serves as a final diagnostic before we move into our Multivariate Regression (OLS) to see which features should be pruned to avoid model instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e00e164-ebc5-45cf-bcd6-ca2a304931c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. Define the Safety Matrix features for analysis\n",
    "# We include densities, buffer counts, and our custom weighted risk score\n",
    "safety_features = [\n",
    "    'crime_density_total','crime_density_violent', 'crime_density_property',\n",
    "    'nb_crimes_half_mile_total','nb_homicides_half_mile', 'nb_crimes_half_mile_violent', \n",
    "    'nb_crimes_half_mile_property', 'weighted_safety_risk_score',\n",
    "    'price_num'\n",
    "]\n",
    "\n",
    "room_types = ['Entire home/apt', 'Private room']\n",
    "\n",
    "print(\"--- Heatmap: Safety Matrix vs. Price ---\")\n",
    "\n",
    "# 2. Create side-by-side correlation heatmaps\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "for i, r_type in enumerate(room_types):\n",
    "    # Filter dataset by room type\n",
    "    subset = listings_cleaned[listings_cleaned['room_type'] == r_type]\n",
    "    \n",
    "    # Calculate the Pearson correlation matrix\n",
    "    corr_matrix = subset[safety_features].corr()\n",
    "    \n",
    "    # Create a mask to hide the upper triangle for better clarity\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    \n",
    "    # Plot heatmap using 'coolwarm' for consistency with previous modules\n",
    "    sns.heatmap(\n",
    "        corr_matrix, \n",
    "        mask=mask,\n",
    "        annot=True,          # Show exact correlation values\n",
    "        cmap='coolwarm',     # Red for positive, Blue for negative (matches Module 2)\n",
    "        fmt=\".2f\",           # 2 decimal places\n",
    "        vmin=-1, vmax=1,     # Standardized correlation scale\n",
    "        linewidths=0.5, \n",
    "        ax=axes[i],\n",
    "        cbar_kws={\"shrink\": 0.8}\n",
    "    )\n",
    "    \n",
    "    axes[i].set_title(f\"Safety Impact: {r_type}\", fontsize=14, pad=15)\n",
    "    \n",
    "    # Rotate x-axis labels to prevent overlap\n",
    "    axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=30, ha='right')\n",
    "\n",
    "plt.suptitle(\"Price Signal & Collinearity: Safety & Risk Features\", fontsize=16, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7785d1a1-40cf-4e0a-ab44-900610a761b6",
   "metadata": {},
   "source": [
    "#### 3.3 Final Feature Convergence: Geographic & Safety Integration\n",
    "To conclude our engineering phase, we perform a strategic pruning of our features. In both the **Lakefront** and **Safety** categories, we observe significant multicollinearity. By retaining only the non-linear gravity scores and specific high-impact risk indicators, we ensure our dataset is optimized for predictive modeling without the noise of redundant distance metrics.\n",
    "\n",
    "**Trial & Error Note:** 1. **Lakefront Consolidation:** We prioritize `shoreline_gravity` over linear distance as it better reflects the exponential value of a lake view. \n",
    "2. **Safety Parsimony:** For crime, we retain `crime_density_total` to represent the general neighborhood reputation and `nb_homicides_half_mile` to capture the most severe localized safety shocks. \n",
    "This balanced selection provides a high-fidelity spatial profile of each listing while maintaining statistical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667ae5b1-5657-453c-becb-484169ffe568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Finalize Feature Selection: Drop redundant Lakefront & Safety columns\n",
    "print(\"--- Executing Final Multi-Scale Feature Selection ---\")\n",
    "\n",
    "# Define all columns to be pruned due to multicollinearity\n",
    "cols_to_drop = [\n",
    "    # Lakefront Redundancies\n",
    "    'dist_to_shoreline_miles', \n",
    "    'shoreline_north_premium',\n",
    "    # Safety Redundancies\n",
    "    'crime_density_violent', \n",
    "    'crime_density_property', \n",
    "    'nb_crimes_half_mile_total', \n",
    "    'nb_crimes_half_mile_violent', \n",
    "    'nb_crimes_half_mile_property', \n",
    "    'weighted_safety_risk_score'\n",
    "]\n",
    "\n",
    "# Execute dropping with check\n",
    "existing_drops = [c for c in cols_to_drop if c in listings_cleaned.columns]\n",
    "listings_cleaned = listings_cleaned.drop(columns=existing_drops)\n",
    "\n",
    "# 2. Validation and Final Dataset Audit\n",
    "print(f\"ðŸ§¹ Successfully pruned {len(existing_drops)} redundant features across Geo and Safety domains.\")\n",
    "print(\"âœ… Final feature set retained for analysis.\")\n",
    "\n",
    "# 3. Final Audit of all engineered features currently in the dataset\n",
    "final_feature_summary = [\n",
    "    'dist_navy_pier_miles', \n",
    "    'is_landmark_proximate', \n",
    "    'transit_north_premium', \n",
    "    'is_premium_transit_hub',\n",
    "    'shoreline_gravity',\n",
    "    'dist_to_nearest_beach_miles',\n",
    "    'crime_density_total',\n",
    "    'nb_homicides_half_mile'\n",
    "]\n",
    "\n",
    "print(\"\\n--- Final Engineered Feature Set ---\")\n",
    "display(listings_cleaned[final_feature_summary].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc26a6-c0dc-4ac3-86a6-ac1f14474948",
   "metadata": {},
   "source": [
    "#### 3.4 Unified Correlation Analysis: The Geo-Safety Matrix\n",
    "This final visualization serves as the \"audit trail\" for our entire feature engineering process. By plotting all remaining curated features against `price_num`, we can observe the collective impact of location, transit, and safety. \n",
    "\n",
    "**Trial & Error Note:** Having pruned the redundant variables in the previous step, this matrix should now be much \"cleaner.\" We are looking for two things:\n",
    "1. **The Price Signal:** Which features (e.g., `shoreline_gravity` or `transit_north_premium`) exhibit the strongest and most consistent relationship with price across room types?\n",
    "2. **Residual Collinearity:** While we've dropped the most obvious culprits, some natural spatial correlations will remain (e.g., landmarks are often near transit). This heatmap confirms that these relationships are now at a manageable level for any future modeling or interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee83c3c-b29a-42e4-803a-30403cb9f04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the final curated feature set for analysis\n",
    "final_analysis_features = [\n",
    "    'dist_navy_pier_miles', \n",
    "    'is_landmark_proximate', \n",
    "    'transit_north_premium', \n",
    "    'is_premium_transit_hub',\n",
    "    'shoreline_gravity',\n",
    "    'dist_to_nearest_beach_miles',\n",
    "    'crime_density_total',\n",
    "    'nb_homicides_half_mile',\n",
    "    'price_num'\n",
    "]\n",
    "\n",
    "room_types = ['Entire home/apt', 'Private room']\n",
    "\n",
    "print(\"--- Final Unified Heatmap: All Engineered Features vs. Price ---\")\n",
    "\n",
    "# 2. Create side-by-side correlation heatmaps\n",
    "fig, axes = plt.subplots(1, 2, figsize=(22, 12))\n",
    "\n",
    "for i, r_type in enumerate(room_types):\n",
    "    # Filter dataset by room type\n",
    "    subset = listings_cleaned[listings_cleaned['room_type'] == r_type]\n",
    "    \n",
    "    # Calculate the Pearson correlation matrix\n",
    "    corr_matrix = subset[final_analysis_features].corr()\n",
    "    \n",
    "    # Create a mask to hide the upper triangle for maximum scannability\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    \n",
    "    # Plot heatmap with 'coolwarm' for consistent branding\n",
    "    sns.heatmap(\n",
    "        corr_matrix, \n",
    "        mask=mask,\n",
    "        annot=True,          # Show coefficients\n",
    "        cmap='coolwarm',     # Red (+) / Blue (-)\n",
    "        fmt=\".2f\",           # 2 decimal places\n",
    "        vmin=-1, vmax=1,     # Fixed scale\n",
    "        linewidths=0.5, \n",
    "        ax=axes[i],\n",
    "        cbar_kws={\"shrink\": 0.8}\n",
    "    )\n",
    "    \n",
    "    axes[i].set_title(f\"Final Geo-Safety Impact: {r_type}\", fontsize=16, pad=20)\n",
    "    \n",
    "    # Optimize label rotation\n",
    "    axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=45, ha='right', fontsize=10)\n",
    "    axes[i].set_yticklabels(axes[i].get_yticklabels(), fontsize=10)\n",
    "\n",
    "plt.suptitle(\"Final Feature Audit: Geospatial & Safety Synergy\", fontsize=20, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e658e8-9284-45f4-99f4-2a0be45479c4",
   "metadata": {},
   "source": [
    "### Data Dictionary: Final Geospatial & Safety Feature Set\n",
    "\n",
    "| Feature Name | Type | Description & Economic Rationale |\n",
    "| :--- | :--- | :--- |\n",
    "| **dist_navy_pier_miles** | Numeric | Straight-line distance to Navy Pier. Acts as a proxy for the listing's proximity to Chicago's primary tourism epicenter. |\n",
    "| **is_landmark_proximate** | Binary | Indicates if a listing is within a critical buffer zone of major Chicago landmarks. Captures the \"prestige premium\" beyond simple distance. |\n",
    "| **transit_north_premium** | Numeric | An interaction term capturing the high-value synergy of being near transit specifically in the affluent North Side. Represents the \"convenience-location\" price boost. |\n",
    "| **is_premium_transit_hub** | Binary | Flags listings located near major transit nodes (e.g., Union Station or key 'L' junctions). Captures utility for business travelers and commuters. |\n",
    "| **shoreline_gravity** | Numeric | A non-linear \"pull\" score for Lake Michigan. Values decay exponentially as distance from the shore increases, modeling the high concentration of value at the waterfront. |\n",
    "| **dist_to_nearest_beach_miles** | Numeric | Distance to the nearest public beach access. Distinguishes purely aesthetic lake views from actual recreational utility and summer seasonal value. |\n",
    "| **crime_density_total** | Numeric | Total crime incidents per square mile at the neighborhood level. Represents the \"general reputation\" and environmental atmosphere of the area. |\n",
    "| **nb_homicides_half_mile** | Numeric | Count of homicides within a 0.5-mile radius. Serves as a high-impact safety signal; used as a primary \"price depressor\" to capture extreme risk sensitivity. |\n",
    "| **price_num** | Numeric | The target variable: Nightly listing price in USD. Cleaned and converted to numeric format for analysis. |\n",
    "\n",
    "---\n",
    "\n",
    "### Modeling Note: Feature Parsimony\n",
    "This feature set has been intentionally pruned to resolve high multicollinearity (VIF reduction). \n",
    "- We prioritize **shoreline_gravity** over raw linear distance to better capture non-linear price appreciation.\n",
    "- We utilize **crime_density_total** to summarize neighborhood-wide trends while using **nb_homicides_half_mile** to capture localized, high-severity psychological deterrents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5719cb37-fc65-40c5-8aca-646210b6aff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 1. Define the final columns for export\n",
    "# Switching 'price_num' back to original 'price'\n",
    "export_columns = [\n",
    "    'id', \n",
    "    'price',\n",
    "    'dist_navy_pier_miles', \n",
    "    'is_landmark_proximate', \n",
    "    'transit_north_premium', \n",
    "    'is_premium_transit_hub',\n",
    "    'shoreline_gravity', \n",
    "    'dist_to_nearest_beach_miles',\n",
    "    'crime_density_total', \n",
    "    'nb_homicides_half_mile'\n",
    "]\n",
    "\n",
    "# 2. Rounding: Ensure all numeric features are capped at 2 decimal places\n",
    "# We only target the float columns to maintain 'id' and binary flags as integers\n",
    "float_features = [\n",
    "    'dist_navy_pier_miles', \n",
    "    'transit_north_premium', \n",
    "    'is_premium_transit_hub',\n",
    "    'shoreline_gravity', \n",
    "    'dist_to_nearest_beach_miles',\n",
    "    'crime_density_total'\n",
    "]\n",
    "\n",
    "# Apply rounding to the dataframe\n",
    "listings_cleaned[float_features] = listings_cleaned[float_features].round(4)\n",
    "\n",
    "# 3. Define output path\n",
    "output_path = \"../../data_processed/Geospatial_Engineering_v3.csv\"\n",
    "\n",
    "print(f\"--- Exporting Refined Feature Set to {output_path} ---\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "output_dir = os.path.dirname(output_path)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# 4. Save to CSV\n",
    "try:\n",
    "    # Exporting the selected subset with 2-decimal precision\n",
    "    listings_cleaned[export_columns].to_csv(output_path, index=False)\n",
    "    print(f\"ðŸ’¾ Successfully saved {len(listings_cleaned)} rows.\")\n",
    "    print(f\"âœ… Data exported with 2-decimal precision and original 'price' column.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during export: {e}\")\n",
    "\n",
    "# Final verification of the rounded output\n",
    "display(listings_cleaned[export_columns].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
